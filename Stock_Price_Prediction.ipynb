{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Price Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMFGaNRIflA",
        "colab_type": "code",
        "outputId": "c75b94a2-684b-4b60-cfb9-9d4f7f5cfc53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "pip install yahoo_fin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yahoo_fin\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/e8/b42de33475fb22ea051753a64c1b101ee901df88013801f722001998b36d/yahoo_fin-0.8.5-py3-none-any.whl\n",
            "Installing collected packages: yahoo-fin\n",
            "Successfully installed yahoo-fin-0.8.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TquM2JI4H9Tc",
        "colab_type": "code",
        "outputId": "8c74a395-7c4d-4e18-fcf0-8d9c1c78410a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from yahoo_fin import stock_info as si\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning - Certain functionality \n",
            "             requires requests_html, which is not installed.\n",
            "             \n",
            "             Install using: \n",
            "             pip install requests_html\n",
            "             \n",
            "             After installation, you may have to restart your Python session.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un2JF5imIJ3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(314)\n",
        "tf.random.set_seed(314)\n",
        "random.seed(314)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiS0b0mZI17w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n",
        "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
        "    # see if ticker is already a loaded stock from yahoo finance\n",
        "    if isinstance(ticker, str):\n",
        "        # load it from yahoo_fin library\n",
        "        df = si.get_data(ticker)\n",
        "    elif isinstance(ticker, pd.DataFrame):\n",
        "        # already loaded, use it directly\n",
        "        df = ticker\n",
        "    # this will contain all the elements we want to return from this function\n",
        "    result = {}\n",
        "    # we will also return the original dataframe itself\n",
        "    result['df'] = df.copy()\n",
        "    # make sure that the passed feature_columns exist in the dataframe\n",
        "    for col in feature_columns:\n",
        "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in feature_columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    # add the target column (label) by shifting by `lookup_step`\n",
        "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
        "    # last `lookup_step` columns contains NaN in future column\n",
        "    # get them before droping NaNs\n",
        "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
        "    # drop NaNs\n",
        "    df.dropna(inplace=True)\n",
        "    sequence_data = []\n",
        "    sequences = deque(maxlen=n_steps)\n",
        "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
        "        sequences.append(entry)\n",
        "        if len(sequences) == n_steps:\n",
        "            sequence_data.append([np.array(sequences), target])\n",
        "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
        "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
        "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
        "    last_sequence = list(sequences) + list(last_sequence)\n",
        "    # shift the last sequence by -1\n",
        "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
        "    # add to result\n",
        "    result['last_sequence'] = last_sequence\n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in sequence_data:\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    # reshape X to fit the neural network\n",
        "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
        "    # split the dataset\n",
        "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
        "    # return the result\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuhDzSnNI_PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
        "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "    model = Sequential()\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            # first layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
        "        elif i == n_layers - 1:\n",
        "            # last layer\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=False))\n",
        "        else:\n",
        "            # hidden layers\n",
        "            if bidirectional:\n",
        "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "            else:\n",
        "                model.add(cell(units, return_sequences=True))\n",
        "        # add dropout after each layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1, activation=\"linear\"))\n",
        "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuADw2HKJKSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Window size or the sequence length\n",
        "N_STEPS = 100\n",
        "# Lookup step, 1 is the next day\n",
        "LOOKUP_STEP = 1\n",
        "# test ratio size, 0.2 is 20%\n",
        "TEST_SIZE = 0.2\n",
        "# features to use\n",
        "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "# date now\n",
        "date_now = time.strftime(\"%Y-%m-%d\")\n",
        "### model parameters\n",
        "N_LAYERS = 3\n",
        "# LSTM cell\n",
        "CELL = LSTM\n",
        "# 256 LSTM neurons\n",
        "UNITS = 256\n",
        "# 40% dropout\n",
        "DROPOUT = 0.4\n",
        "# whether to use bidirectional RNNs\n",
        "BIDIRECTIONAL = False\n",
        "### training parameters\n",
        "# mean absolute error loss\n",
        "# LOSS = \"mae\"\n",
        "# huber loss\n",
        "LOSS = \"huber_loss\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100   # Increase epochs to 400 to obtain accuracy upto 80% - but it will consume lots of hours\n",
        "# Apple stock market\n",
        "ticker = \"AAPL\"\n",
        "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
        "# model name to save, making it as unique as possible based on parameters\n",
        "model_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
        "if BIDIRECTIONAL:\n",
        "    model_name += \"-b\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5YKwPaIJRJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create these folders if they does not exist\n",
        "if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "if not os.path.isdir(\"logs\"):\n",
        "    os.mkdir(\"logs\")\n",
        "if not os.path.isdir(\"data\"):\n",
        "    os.mkdir(\"data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgdkPMBVJXkO",
        "colab_type": "code",
        "outputId": "6b1e8fa3-b9b8-4824-9162-282215729f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load the data\n",
        "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
        "\n",
        "# save the dataframe\n",
        "data[\"df\"].to_csv(ticker_data_filename)\n",
        "\n",
        "# construct the model\n",
        "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "\n",
        "# some tensorflow callbacks\n",
        "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[checkpointer, tensorboard],\n",
        "                    verbose=1)\n",
        "\n",
        "model.save(os.path.join(\"results\", model_name) + \".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0208\n",
            "Epoch 00001: val_loss improved from inf to 0.00066, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 16s 126ms/step - loss: 0.0012 - mean_absolute_error: 0.0208 - val_loss: 6.6150e-04 - val_mean_absolute_error: 0.0157\n",
            "Epoch 2/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 7.5357e-04 - mean_absolute_error: 0.0180\n",
            "Epoch 00002: val_loss did not improve from 0.00066\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 7.5357e-04 - mean_absolute_error: 0.0180 - val_loss: 8.9474e-04 - val_mean_absolute_error: 0.0176\n",
            "Epoch 3/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 6.5510e-04 - mean_absolute_error: 0.0166\n",
            "Epoch 00003: val_loss improved from 0.00066 to 0.00045, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 6.5510e-04 - mean_absolute_error: 0.0166 - val_loss: 4.5138e-04 - val_mean_absolute_error: 0.0120\n",
            "Epoch 4/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 5.0269e-04 - mean_absolute_error: 0.0147\n",
            "Epoch 00004: val_loss did not improve from 0.00045\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 5.0269e-04 - mean_absolute_error: 0.0147 - val_loss: 6.5915e-04 - val_mean_absolute_error: 0.0168\n",
            "Epoch 5/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 5.4623e-04 - mean_absolute_error: 0.0156\n",
            "Epoch 00005: val_loss improved from 0.00045 to 0.00017, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 117ms/step - loss: 5.4623e-04 - mean_absolute_error: 0.0156 - val_loss: 1.6985e-04 - val_mean_absolute_error: 0.0086\n",
            "Epoch 6/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 4.9960e-04 - mean_absolute_error: 0.0150\n",
            "Epoch 00006: val_loss improved from 0.00017 to 0.00015, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 4.9960e-04 - mean_absolute_error: 0.0150 - val_loss: 1.4908e-04 - val_mean_absolute_error: 0.0073\n",
            "Epoch 7/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 3.6852e-04 - mean_absolute_error: 0.0129\n",
            "Epoch 00007: val_loss did not improve from 0.00015\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 3.6852e-04 - mean_absolute_error: 0.0129 - val_loss: 2.9645e-04 - val_mean_absolute_error: 0.0134\n",
            "Epoch 8/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 4.0653e-04 - mean_absolute_error: 0.0135\n",
            "Epoch 00008: val_loss did not improve from 0.00015\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 4.0653e-04 - mean_absolute_error: 0.0135 - val_loss: 4.2084e-04 - val_mean_absolute_error: 0.0137\n",
            "Epoch 9/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 3.8213e-04 - mean_absolute_error: 0.0132\n",
            "Epoch 00009: val_loss did not improve from 0.00015\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 3.8213e-04 - mean_absolute_error: 0.0132 - val_loss: 0.0012 - val_mean_absolute_error: 0.0275\n",
            "Epoch 10/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 3.8951e-04 - mean_absolute_error: 0.0137\n",
            "Epoch 00010: val_loss improved from 0.00015 to 0.00008, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 3.8951e-04 - mean_absolute_error: 0.0137 - val_loss: 7.7558e-05 - val_mean_absolute_error: 0.0064\n",
            "Epoch 11/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.8920e-04 - mean_absolute_error: 0.0121\n",
            "Epoch 00011: val_loss improved from 0.00008 to 0.00007, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 123ms/step - loss: 2.8920e-04 - mean_absolute_error: 0.0121 - val_loss: 6.7832e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 12/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.6712e-04 - mean_absolute_error: 0.0116\n",
            "Epoch 00012: val_loss did not improve from 0.00007\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 2.6712e-04 - mean_absolute_error: 0.0116 - val_loss: 1.5015e-04 - val_mean_absolute_error: 0.0092\n",
            "Epoch 13/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.6549e-04 - mean_absolute_error: 0.0120\n",
            "Epoch 00013: val_loss did not improve from 0.00007\n",
            "124/124 [==============================] - 15s 122ms/step - loss: 2.6549e-04 - mean_absolute_error: 0.0120 - val_loss: 1.1550e-04 - val_mean_absolute_error: 0.0065\n",
            "Epoch 14/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.4513e-04 - mean_absolute_error: 0.0115\n",
            "Epoch 00014: val_loss improved from 0.00007 to 0.00003, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 2.4513e-04 - mean_absolute_error: 0.0115 - val_loss: 3.4528e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 15/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.5141e-04 - mean_absolute_error: 0.0115\n",
            "Epoch 00015: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 117ms/step - loss: 2.5141e-04 - mean_absolute_error: 0.0115 - val_loss: 1.4073e-04 - val_mean_absolute_error: 0.0111\n",
            "Epoch 16/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.2322e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 00016: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 121ms/step - loss: 2.2322e-04 - mean_absolute_error: 0.0112 - val_loss: 1.1911e-04 - val_mean_absolute_error: 0.0093\n",
            "Epoch 17/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.1892e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00017: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 117ms/step - loss: 2.1892e-04 - mean_absolute_error: 0.0109 - val_loss: 3.5023e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 18/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.8478e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00018: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 117ms/step - loss: 1.8478e-04 - mean_absolute_error: 0.0104 - val_loss: 1.1691e-04 - val_mean_absolute_error: 0.0060\n",
            "Epoch 19/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.1123e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00019: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 2.1123e-04 - mean_absolute_error: 0.0109 - val_loss: 5.0713e-05 - val_mean_absolute_error: 0.0056\n",
            "Epoch 20/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.3420e-04 - mean_absolute_error: 0.0116\n",
            "Epoch 00020: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 2.3420e-04 - mean_absolute_error: 0.0116 - val_loss: 8.5300e-05 - val_mean_absolute_error: 0.0068\n",
            "Epoch 21/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.0656e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00021: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 2.0656e-04 - mean_absolute_error: 0.0108 - val_loss: 6.0001e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 22/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.9682e-04 - mean_absolute_error: 0.0112\n",
            "Epoch 00022: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 1.9682e-04 - mean_absolute_error: 0.0112 - val_loss: 4.0160e-05 - val_mean_absolute_error: 0.0067\n",
            "Epoch 23/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5720e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00023: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 1.5720e-04 - mean_absolute_error: 0.0098 - val_loss: 4.3389e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 24/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.9551e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00024: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 1.9551e-04 - mean_absolute_error: 0.0109 - val_loss: 5.2670e-05 - val_mean_absolute_error: 0.0078\n",
            "Epoch 25/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.8967e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00025: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 1.8967e-04 - mean_absolute_error: 0.0108 - val_loss: 1.6710e-04 - val_mean_absolute_error: 0.0089\n",
            "Epoch 26/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.3140e-04 - mean_absolute_error: 0.0122\n",
            "Epoch 00026: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 121ms/step - loss: 2.3140e-04 - mean_absolute_error: 0.0122 - val_loss: 5.3102e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 27/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6769e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00027: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 122ms/step - loss: 1.6769e-04 - mean_absolute_error: 0.0102 - val_loss: 5.4552e-05 - val_mean_absolute_error: 0.0048\n",
            "Epoch 28/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.7490e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00028: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 1.7490e-04 - mean_absolute_error: 0.0104 - val_loss: 2.1805e-04 - val_mean_absolute_error: 0.0150\n",
            "Epoch 29/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.7622e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00029: val_loss did not improve from 0.00003\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 1.7622e-04 - mean_absolute_error: 0.0109 - val_loss: 3.5329e-04 - val_mean_absolute_error: 0.0130\n",
            "Epoch 30/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.1399e-04 - mean_absolute_error: 0.0117\n",
            "Epoch 00030: val_loss improved from 0.00003 to 0.00003, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 2.1399e-04 - mean_absolute_error: 0.0117 - val_loss: 2.6215e-05 - val_mean_absolute_error: 0.0034\n",
            "Epoch 31/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4818e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00031: val_loss improved from 0.00003 to 0.00002, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 117ms/step - loss: 1.4818e-04 - mean_absolute_error: 0.0098 - val_loss: 2.3494e-05 - val_mean_absolute_error: 0.0043\n",
            "Epoch 32/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6172e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00032: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 1.6172e-04 - mean_absolute_error: 0.0105 - val_loss: 3.4917e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 33/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3534e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00033: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 1.3534e-04 - mean_absolute_error: 0.0094 - val_loss: 5.3952e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 34/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4691e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00034: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 1.4691e-04 - mean_absolute_error: 0.0101 - val_loss: 8.2238e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 35/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6081e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00035: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 1.6081e-04 - mean_absolute_error: 0.0101 - val_loss: 3.2288e-05 - val_mean_absolute_error: 0.0064\n",
            "Epoch 36/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.7842e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 00036: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 1.7842e-04 - mean_absolute_error: 0.0107 - val_loss: 2.2086e-05 - val_mean_absolute_error: 0.0036\n",
            "Epoch 37/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5512e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00037: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 119ms/step - loss: 1.5512e-04 - mean_absolute_error: 0.0099 - val_loss: 3.5350e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 38/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2983e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00038: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 1.2983e-04 - mean_absolute_error: 0.0092 - val_loss: 1.1329e-04 - val_mean_absolute_error: 0.0080\n",
            "Epoch 39/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6358e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00039: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 117ms/step - loss: 1.6358e-04 - mean_absolute_error: 0.0105 - val_loss: 4.2177e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 40/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6445e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00040: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 1.6445e-04 - mean_absolute_error: 0.0104 - val_loss: 4.2526e-05 - val_mean_absolute_error: 0.0078\n",
            "Epoch 41/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.9794e-04 - mean_absolute_error: 0.0114\n",
            "Epoch 00041: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 118ms/step - loss: 1.9794e-04 - mean_absolute_error: 0.0114 - val_loss: 2.9923e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 42/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6203e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00042: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 117ms/step - loss: 1.6203e-04 - mean_absolute_error: 0.0102 - val_loss: 1.6781e-04 - val_mean_absolute_error: 0.0093\n",
            "Epoch 43/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.8221e-04 - mean_absolute_error: 0.0110\n",
            "Epoch 00043: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.8221e-04 - mean_absolute_error: 0.0110 - val_loss: 1.4121e-04 - val_mean_absolute_error: 0.0144\n",
            "Epoch 44/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 2.0561e-04 - mean_absolute_error: 0.0118\n",
            "Epoch 00044: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 2.0561e-04 - mean_absolute_error: 0.0118 - val_loss: 4.2211e-05 - val_mean_absolute_error: 0.0072\n",
            "Epoch 45/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6759e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00045: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.6759e-04 - mean_absolute_error: 0.0105 - val_loss: 3.7790e-05 - val_mean_absolute_error: 0.0063\n",
            "Epoch 46/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4963e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00046: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.4963e-04 - mean_absolute_error: 0.0100 - val_loss: 5.8855e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 47/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4533e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00047: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.4533e-04 - mean_absolute_error: 0.0097 - val_loss: 2.8418e-05 - val_mean_absolute_error: 0.0056\n",
            "Epoch 48/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.7302e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00048: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 15s 120ms/step - loss: 1.7302e-04 - mean_absolute_error: 0.0105 - val_loss: 5.3698e-05 - val_mean_absolute_error: 0.0070\n",
            "Epoch 49/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4955e-04 - mean_absolute_error: 0.0104\n",
            "Epoch 00049: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.4955e-04 - mean_absolute_error: 0.0104 - val_loss: 3.8985e-05 - val_mean_absolute_error: 0.0052\n",
            "Epoch 50/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3091e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00050: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.3091e-04 - mean_absolute_error: 0.0094 - val_loss: 2.2759e-05 - val_mean_absolute_error: 0.0035\n",
            "Epoch 51/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.7010e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00051: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.7010e-04 - mean_absolute_error: 0.0106 - val_loss: 6.5203e-05 - val_mean_absolute_error: 0.0053\n",
            "Epoch 52/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6668e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00052: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.6668e-04 - mean_absolute_error: 0.0106 - val_loss: 2.8528e-05 - val_mean_absolute_error: 0.0035\n",
            "Epoch 53/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4415e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00053: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 112ms/step - loss: 1.4415e-04 - mean_absolute_error: 0.0097 - val_loss: 2.2799e-05 - val_mean_absolute_error: 0.0035\n",
            "Epoch 54/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3628e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00054: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.3628e-04 - mean_absolute_error: 0.0097 - val_loss: 5.9417e-05 - val_mean_absolute_error: 0.0073\n",
            "Epoch 55/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4350e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00055: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.4350e-04 - mean_absolute_error: 0.0098 - val_loss: 8.1000e-05 - val_mean_absolute_error: 0.0109\n",
            "Epoch 56/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3948e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00056: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.3948e-04 - mean_absolute_error: 0.0100 - val_loss: 5.9878e-05 - val_mean_absolute_error: 0.0069\n",
            "Epoch 57/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2221e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00057: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.2221e-04 - mean_absolute_error: 0.0092 - val_loss: 3.0762e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 58/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4966e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00058: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.4966e-04 - mean_absolute_error: 0.0098 - val_loss: 3.4826e-05 - val_mean_absolute_error: 0.0041\n",
            "Epoch 59/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3862e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00059: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.3862e-04 - mean_absolute_error: 0.0096 - val_loss: 5.1771e-05 - val_mean_absolute_error: 0.0070\n",
            "Epoch 60/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5991e-04 - mean_absolute_error: 0.0105\n",
            "Epoch 00060: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.5991e-04 - mean_absolute_error: 0.0105 - val_loss: 2.9565e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 61/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.7752e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00061: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.7752e-04 - mean_absolute_error: 0.0109 - val_loss: 3.3968e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 62/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4018e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00062: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.4018e-04 - mean_absolute_error: 0.0096 - val_loss: 9.6967e-05 - val_mean_absolute_error: 0.0073\n",
            "Epoch 63/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3999e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00063: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.3999e-04 - mean_absolute_error: 0.0099 - val_loss: 2.1320e-05 - val_mean_absolute_error: 0.0031\n",
            "Epoch 64/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3666e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00064: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.3666e-04 - mean_absolute_error: 0.0094 - val_loss: 3.9639e-05 - val_mean_absolute_error: 0.0063\n",
            "Epoch 65/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4624e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00065: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.4624e-04 - mean_absolute_error: 0.0100 - val_loss: 2.5929e-05 - val_mean_absolute_error: 0.0034\n",
            "Epoch 66/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.8041e-04 - mean_absolute_error: 0.0107\n",
            "Epoch 00066: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.8041e-04 - mean_absolute_error: 0.0107 - val_loss: 1.1178e-04 - val_mean_absolute_error: 0.0083\n",
            "Epoch 67/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5060e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00067: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.5060e-04 - mean_absolute_error: 0.0102 - val_loss: 2.9246e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 68/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5710e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00068: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.5710e-04 - mean_absolute_error: 0.0103 - val_loss: 1.0830e-04 - val_mean_absolute_error: 0.0096\n",
            "Epoch 69/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3310e-04 - mean_absolute_error: 0.0095\n",
            "Epoch 00069: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.3310e-04 - mean_absolute_error: 0.0095 - val_loss: 1.2108e-04 - val_mean_absolute_error: 0.0077\n",
            "Epoch 70/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5572e-04 - mean_absolute_error: 0.0106\n",
            "Epoch 00070: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.5572e-04 - mean_absolute_error: 0.0106 - val_loss: 3.1967e-05 - val_mean_absolute_error: 0.0062\n",
            "Epoch 71/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2813e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00071: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.2813e-04 - mean_absolute_error: 0.0093 - val_loss: 4.7584e-05 - val_mean_absolute_error: 0.0055\n",
            "Epoch 72/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.6766e-04 - mean_absolute_error: 0.0108\n",
            "Epoch 00072: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.6766e-04 - mean_absolute_error: 0.0108 - val_loss: 2.6762e-05 - val_mean_absolute_error: 0.0042\n",
            "Epoch 73/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3124e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00073: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.3124e-04 - mean_absolute_error: 0.0097 - val_loss: 4.0778e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 74/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2597e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00074: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.2597e-04 - mean_absolute_error: 0.0093 - val_loss: 4.0385e-05 - val_mean_absolute_error: 0.0077\n",
            "Epoch 75/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2464e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00075: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.2464e-04 - mean_absolute_error: 0.0092 - val_loss: 2.9005e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 76/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2411e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00076: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.2411e-04 - mean_absolute_error: 0.0092 - val_loss: 1.5032e-04 - val_mean_absolute_error: 0.0110\n",
            "Epoch 77/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5729e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00077: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.5729e-04 - mean_absolute_error: 0.0102 - val_loss: 1.8919e-05 - val_mean_absolute_error: 0.0037\n",
            "Epoch 78/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2704e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00078: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.2704e-04 - mean_absolute_error: 0.0093 - val_loss: 3.0008e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 79/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.1478e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00079: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 116ms/step - loss: 1.1478e-04 - mean_absolute_error: 0.0090 - val_loss: 2.8947e-05 - val_mean_absolute_error: 0.0038\n",
            "Epoch 80/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4203e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00080: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.4203e-04 - mean_absolute_error: 0.0097 - val_loss: 2.2652e-05 - val_mean_absolute_error: 0.0034\n",
            "Epoch 81/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3192e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00081: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 115ms/step - loss: 1.3192e-04 - mean_absolute_error: 0.0094 - val_loss: 2.3492e-05 - val_mean_absolute_error: 0.0039\n",
            "Epoch 82/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3028e-04 - mean_absolute_error: 0.0092\n",
            "Epoch 00082: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.3028e-04 - mean_absolute_error: 0.0092 - val_loss: 7.4806e-05 - val_mean_absolute_error: 0.0092\n",
            "Epoch 83/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2932e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00083: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 113ms/step - loss: 1.2932e-04 - mean_absolute_error: 0.0097 - val_loss: 1.4368e-04 - val_mean_absolute_error: 0.0081\n",
            "Epoch 84/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2915e-04 - mean_absolute_error: 0.0093\n",
            "Epoch 00084: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 114ms/step - loss: 1.2915e-04 - mean_absolute_error: 0.0093 - val_loss: 2.8274e-05 - val_mean_absolute_error: 0.0033\n",
            "Epoch 85/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5226e-04 - mean_absolute_error: 0.0102\n",
            "Epoch 00085: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 112ms/step - loss: 1.5226e-04 - mean_absolute_error: 0.0102 - val_loss: 3.4924e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 86/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4609e-04 - mean_absolute_error: 0.0100\n",
            "Epoch 00086: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 110ms/step - loss: 1.4609e-04 - mean_absolute_error: 0.0100 - val_loss: 3.8186e-05 - val_mean_absolute_error: 0.0060\n",
            "Epoch 87/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.7047e-04 - mean_absolute_error: 0.0101\n",
            "Epoch 00087: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 108ms/step - loss: 1.7047e-04 - mean_absolute_error: 0.0101 - val_loss: 3.3253e-05 - val_mean_absolute_error: 0.0066\n",
            "Epoch 88/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3669e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00088: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 14s 109ms/step - loss: 1.3669e-04 - mean_absolute_error: 0.0096 - val_loss: 2.5579e-05 - val_mean_absolute_error: 0.0044\n",
            "Epoch 89/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4443e-04 - mean_absolute_error: 0.0097\n",
            "Epoch 00089: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 107ms/step - loss: 1.4443e-04 - mean_absolute_error: 0.0097 - val_loss: 4.7434e-05 - val_mean_absolute_error: 0.0061\n",
            "Epoch 90/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2299e-04 - mean_absolute_error: 0.0090\n",
            "Epoch 00090: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 108ms/step - loss: 1.2299e-04 - mean_absolute_error: 0.0090 - val_loss: 2.4403e-05 - val_mean_absolute_error: 0.0046\n",
            "Epoch 91/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.1507e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00091: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 107ms/step - loss: 1.1507e-04 - mean_absolute_error: 0.0091 - val_loss: 3.8203e-05 - val_mean_absolute_error: 0.0051\n",
            "Epoch 92/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.2361e-04 - mean_absolute_error: 0.0094\n",
            "Epoch 00092: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 109ms/step - loss: 1.2361e-04 - mean_absolute_error: 0.0094 - val_loss: 5.0873e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 93/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3302e-04 - mean_absolute_error: 0.0096\n",
            "Epoch 00093: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 106ms/step - loss: 1.3302e-04 - mean_absolute_error: 0.0096 - val_loss: 3.1571e-05 - val_mean_absolute_error: 0.0054\n",
            "Epoch 94/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.5169e-04 - mean_absolute_error: 0.0103\n",
            "Epoch 00094: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 106ms/step - loss: 1.5169e-04 - mean_absolute_error: 0.0103 - val_loss: 5.2509e-05 - val_mean_absolute_error: 0.0049\n",
            "Epoch 95/100\n",
            "123/124 [============================>.] - ETA: 0s - loss: 1.2041e-04 - mean_absolute_error: 0.0091\n",
            "Epoch 00095: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 106ms/step - loss: 1.2037e-04 - mean_absolute_error: 0.0091 - val_loss: 1.9148e-05 - val_mean_absolute_error: 0.0045\n",
            "Epoch 96/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.4325e-04 - mean_absolute_error: 0.0098\n",
            "Epoch 00096: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 107ms/step - loss: 1.4325e-04 - mean_absolute_error: 0.0098 - val_loss: 6.1108e-05 - val_mean_absolute_error: 0.0091\n",
            "Epoch 97/100\n",
            "123/124 [============================>.] - ETA: 0s - loss: 1.5482e-04 - mean_absolute_error: 0.0109\n",
            "Epoch 00097: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 106ms/step - loss: 1.5478e-04 - mean_absolute_error: 0.0109 - val_loss: 4.4711e-05 - val_mean_absolute_error: 0.0057\n",
            "Epoch 98/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.3929e-04 - mean_absolute_error: 0.0099\n",
            "Epoch 00098: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 107ms/step - loss: 1.3929e-04 - mean_absolute_error: 0.0099 - val_loss: 4.4081e-05 - val_mean_absolute_error: 0.0059\n",
            "Epoch 99/100\n",
            "124/124 [==============================] - ETA: 0s - loss: 1.0796e-04 - mean_absolute_error: 0.0089\n",
            "Epoch 00099: val_loss improved from 0.00002 to 0.00002, saving model to results/2020-06-03_AAPL-huber_loss-adam-LSTM-seq-100-step-1-layers-3-units-256.h5\n",
            "124/124 [==============================] - 13s 107ms/step - loss: 1.0796e-04 - mean_absolute_error: 0.0089 - val_loss: 1.5960e-05 - val_mean_absolute_error: 0.0032\n",
            "Epoch 100/100\n",
            "123/124 [============================>.] - ETA: 0s - loss: 1.0963e-04 - mean_absolute_error: 0.0088\n",
            "Epoch 00100: val_loss did not improve from 0.00002\n",
            "124/124 [==============================] - 13s 106ms/step - loss: 1.0966e-04 - mean_absolute_error: 0.0088 - val_loss: 2.5871e-05 - val_mean_absolute_error: 0.0043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-uODmaOJeKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
        "                feature_columns=FEATURE_COLUMNS, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS2hCO5-PU-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
        "                  #  dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
        "\n",
        "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
        "model.load_weights(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1y7GSr5Ptp-",
        "colab_type": "code",
        "outputId": "7ba5cc93-bc3f-49e2-8cf7-10250a86548f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
        "# calculate the mean absolute error (inverse scaling)\n",
        "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 3.2594923328609546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcAu40UVPyT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, data, classification=False):\n",
        "    # retrieve the last sequence from data\n",
        "    last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
        "    # retrieve the column scalers\n",
        "    column_scaler = data[\"column_scaler\"]\n",
        "    # reshape the last sequence\n",
        "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
        "    # expand dimension\n",
        "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
        "    # get the prediction (scaled from 0 to 1)\n",
        "    prediction = model.predict(last_sequence)\n",
        "    # get the price (by inverting the scaling)\n",
        "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
        "    return predicted_price"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlAIVjDYP4i8",
        "colab_type": "code",
        "outputId": "86f6c8d0-9d3a-470c-a1cb-97221f75cad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# predict the future price\n",
        "future_price = predict(model, data)\n",
        "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Future price after 1 days is 330.68$\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMynOcvwP9BP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_graph(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    # last 200 days, feel free to edit that\n",
        "    plt.plot(y_test[-200:], c='b')\n",
        "    plt.plot(y_pred[-200:], c='r')\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZlXsXd9QCrF",
        "colab_type": "code",
        "outputId": "4ac3e359-296f-43e8-92a5-2b4f42e8504f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plot_graph(model, data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coHX2py3QGwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy(model, data):\n",
        "    y_test = data[\"y_test\"]\n",
        "    X_test = data[\"X_test\"]\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
        "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
        "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
        "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
        "    return accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5PwKCjYQR6t",
        "colab_type": "code",
        "outputId": "146a344b-d6e4-4dc2-9ec8-123b347e8991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "3 Improve accuracy if required by increasing number of echos above\n",
        "print(str(LOOKUP_STEP) + \":\", \"Accuracy Score:\", get_accuracy(model, data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: Accuracy Score: 0.5088877602844083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVO3__UeQXDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}